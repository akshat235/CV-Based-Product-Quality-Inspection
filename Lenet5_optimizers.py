# -*- coding: utf-8 -*-
"""IP31-Optimizers

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1H1ZQg3clKQ6Pv3SK7fY1atI1uVKq3BYd

##Data
"""

# Commented out IPython magic to ensure Python compatibility.
import os
import shutil
import random
import itertools
import numpy as np
import pandas as pd
import keras.metrics
from PIL import Image
from skimage import io
import tensorflow as tf
from tensorflow import keras
from keras import layers
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad
from tensorflow.keras.models import Sequential
from tensorflow.keras.metrics import categorical_crossentropy
from tensorflow.keras.layers import Activation, Dense, Flatten, Conv2D, MaxPool2D, AveragePooling2D
from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, array_to_img
# %matplotlib inline

!gdown --id "1-txGiJZhCBQJs6a7XtEc7_7CmpbN86Tz"
!unzip *zip

def plot_cm(cm, classes, normalize = False , title = 'Confusion Matrix', cmap = plt.cm.Blues):
  plt.imshow(cm, interpolation = 'nearest', cmap = cmap)
  plt.title(title)
  plt.colorbar()
  tick_marks = np.arange (len(classes))
  plt.xticks(tick_marks, classes, rotation = 45)
  plt.yticks(tick_marks, classes)

  if normalize:
    cm = cm.astype('float')/cm.sum(axis =1)[:,np.newaxis]
    print("Normalized CM")
  else:
    print ('Confusion Matrix, without normalization')
  print(cm)

  thresh  = cm.max()/2
  for i,j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
    plt.text(j,i,cm[i,j],horizontalalignment = 'center', color = 'white' if cm[i,j]>thresh else"black")

  plt.tight_layout()
  plt.ylabel('True Label')
  plt.xlabel('Predicted Label')

data300 = 'casting_data/casting_data'
train_dir = data300+'/train'
test_dir = data300+'/test'
val_dir = data300 + '/validation'

val_ok = data300 + '/validation/ok_front'
val_def = data300 + '/validation/def_front'

train_def_300 = data300+'/train/def_front'
train_ok_300 = data300+'/train/ok_front'

test_def_300 = data300+'/test/def_front'
test_ok_300 = data300+'/test/ok_front'

print(' No. of images in train_def_300 : ', len(os.listdir(train_def_300)))
print(' No. of images in train_ok_300 :  ' , len(os.listdir(train_ok_300)))

print(' No. of images in test_def_300 :  ', len(os.listdir(test_def_300)))
print(' No. of images in test_ok_300 :   ', len(os.listdir(test_ok_300)))

temp1 = 0
temp2 = 0
os.makedirs(val_def)
os.makedirs(val_ok)

#moving 654 images to val_ok

if temp1 ==0:
  for i in range(654):
    img_name = random.choice(os.listdir(train_ok_300))
    img_path = train_ok_300+'/'+img_name 
    shutil.move(img_path, val_ok+'/'+img_name)
    temp1 +=1
else:
  print('already performed this operation')

#moving 1090 images to val_def

if temp2 ==0:
  for i in range(1090):
    img_name = random.choice(os.listdir(train_def_300))
    img_path = train_def_300+'/'+img_name 
    shutil.move(img_path, val_def+'/'+img_name)
    temp1 +=1
else:
  print('already performed this operation')

assert len(os.listdir(val_def)) == 1090
assert len(os.listdir(val_ok)) == 654

train_batches = ImageDataGenerator(preprocessing_function= tf.keras.applications.vgg16.preprocess_input).flow_from_directory(directory = train_dir,
                                                                                target_size = (224,224), classes = ['def_front','ok_front'], batch_size =32)

validation_batches = ImageDataGenerator(preprocessing_function= tf.keras.applications.vgg16.preprocess_input).flow_from_directory(directory = val_dir,
                                                                                target_size = (224,224), classes = ['def_front','ok_front'], batch_size =32)

test_batches = ImageDataGenerator(preprocessing_function= tf.keras.applications.vgg16.preprocess_input).flow_from_directory(directory = test_dir,
                                                                                target_size = (224,224), classes = ['def_front','ok_front'], batch_size =32, shuffle = False)

"""##Using Adam"""



def make_lenet_model():
  model1 = keras.Sequential()

  model1.add(layers.Conv2D(filters=6, kernel_size=(5, 5), activation='tanh', input_shape=(32,32,1), padding='same'))
  model1.add(layers.AveragePooling2D(padding='same', strides=2))

  model1.add(Conv2D(filters=16, kernel_size=(5, 5), activation='tanh', padding ='same' , strides=1))
  model1.add(AveragePooling2D(padding='same', strides=2))

  model1.add(Flatten())

  model1.add(Dense(units=120, activation='relu'))

  model1.add(Dense(units=84, activation='relu'))

  model1.add(Dense(units=2, activation = 'softmax'))
  return model1

model1 = make_lenet_model()

model1.build((32,224,224,3))

model1.summary()

model1.compile(optimizer=Adam(learning_rate=0.001),metrics = ['accuracy'], loss='categorical_crossentropy')

hist1 = model1.fit(x=train_batches, validation_data = validation_batches, epochs = 25, verbose =1)

pd.DataFrame(hist1.history).plot(figsize = (10,6))
plt.show()

prediction1 = model1.predict(test_batches, verbose =1)

cm = confusion_matrix(y_true=test_batches.classes, y_pred = np.argmax(prediction1, axis=-1))

cm_plotlabels = ['defective','ok']
plot_cm(cm, classes = cm_plotlabels )



"""##Using SGD"""

model2 = make_lenet_model()

model2.build((32,224,224,3))

model2.summary()

model2.compile(optimizer= SGD(learning_rate=0.001),metrics = ['accuracy'], loss='categorical_crossentropy')

hist2 = model2.fit(x=train_batches, validation_data = validation_batches, epochs = 20, verbose =1)

pd.DataFrame(hist2.history).plot(figsize = (10,6))
plt.show()

prediction2 = model2.predict(test_batches, verbose =1)

cm2 = confusion_matrix(y_true=test_batches.classes, y_pred = np.argmax(prediction2, axis=-1))

cm_plotlabels = ['defective','ok']
plot_cm(cm2, classes = cm_plotlabels )



"""###Using SGD with momentum"""

model3 = make_lenet_model()

model3.build((32,224,224,3))

model3.summary()

model3.compile(optimizer= SGD(learning_rate=0.001, momentum=0.9),metrics = ['accuracy'], loss='categorical_crossentropy')

hist3 = model3.fit(x=train_batches, validation_data = validation_batches, epochs = 15, verbose =1)

pd.DataFrame(hist3.history).plot(figsize = (10,6))
plt.show()

prediction3 = model3.predict(test_batches, verbose =1)

cm3 = confusion_matrix(y_true=test_batches.classes, y_pred = np.argmax(prediction2, axis=-1))

cm_plotlabels = ['defective','ok']
plot_cm(cm3, classes = cm_plotlabels )



"""## Using RMSprop"""

model4 = make_lenet_model()
model4.build((32,224,224,3))

model4.summary()

model4.compile(optimizer=RMSprop(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])

hist4=model4.fit(train_batches, validation_data=validation_batches, epochs = 20, verbose = 1 )

pd.DataFrame(hist4.history).plot(figsize=(10,6))
plt.show()

prediction4 = model4.predict(test_batches, verbose=0)

cm = confusion_matrix(y_true=test_batches.classes, y_pred = np.argmax(prediction4, axis=-1))

cm_plotlabels = ['defective','ok']
plot_cm(cm, classes = cm_plotlabels )



"""## Using Adagrad"""

model5=make_lenet_model()
model5.build((32,224,224,3))

model5.compile(Adagrad(learning_rate=0.001), loss = 'categorical_crossentropy', metrics = ['accuracy'])

hist5 = model5.fit(train_batches,  validation_data = validation_batches, epochs = 15, verbose =1)

pd.DataFrame(hist5.history).plot(figsize = (10,6))
plt.show()

prediction5 = model5.predict(test_batches)

cm = confusion_matrix(y_true=test_batches.classes, y_pred = np.argmax(prediction5, axis=-1))

cm_plotlabels = ['defective','ok']
plot_cm(cm, classes = cm_plotlabels )





"""##Using Adam"""



def make_lenet_model():
  model1 = keras.Sequential()

  model1.add(layers.Conv2D(filters=6, kernel_size=(5, 5), activation='tanh', input_shape=(32,32,1), padding='same'))
  model1.add(layers.AveragePooling2D(padding='same', strides=2))

  model1.add(Conv2D(filters=16, kernel_size=(5, 5), activation='tanh', padding ='same' , strides=1))
  model1.add(AveragePooling2D(padding='same', strides=2))

  model1.add(Flatten())

  model1.add(Dense(units=120, activation='relu'))

  model1.add(Dense(units=84, activation='relu'))

  model1.add(Dense(units=2, activation = 'softmax'))
  return model1

model1 = make_lenet_model()

model1.build((32,224,224,3))

model1.summary()

model1.compile(optimizer=Adam(learning_rate = 0.0001),metrics = ['accuracy'], loss='categorical_crossentropy')

hist1 = model1.fit(x=train_batches, validation_data = validation_batches, epochs = 15, verbose =1)

pd.DataFrame(hist1.history).plot(figsize = (10,6))
plt.show()

prediction1 = model1.predict(test_batches, verbose =1)

cm = confusion_matrix(y_true=test_batches.classes, y_pred = np.argmax(prediction1, axis=-1))

cm_plotlabels = ['defective','ok']
plot_cm(cm, classes = cm_plotlabels )



"""##Using Adadelta"""



def make_lenet_model():
  model1 = keras.Sequential()

  model1.add(layers.Conv2D(filters=6, kernel_size=(5, 5), activation='tanh', input_shape=(32,32,1), padding='same'))
  model1.add(layers.AveragePooling2D(padding='same', strides=2))

  model1.add(Conv2D(filters=16, kernel_size=(5, 5), activation='tanh', padding ='same' , strides=1))
  model1.add(AveragePooling2D(padding='same', strides=2))

  model1.add(Flatten())

  model1.add(Dense(units=120, activation='relu'))

  model1.add(Dense(units=84, activation='relu'))

  model1.add(Dense(units=2, activation = 'softmax'))
  return model1

model9 = make_lenet_model()

model9.build((32,224,224,3))

model9.summary()

model9.compile(optimizer=Adadelta(),metrics = ['accuracy'], loss='categorical_crossentropy')

hist9 = model9.fit(x=train_batches, validation_data = validation_batches, epochs = 30, verbose =1)

pd.DataFrame(hist9.history).plot(figsize = (10,6))
plt.show()

prediction9 = model9.predict(test_batches, verbose =1)

cm9 = confusion_matrix(y_true=test_batches.classes, y_pred = np.argmax(prediction9, axis=-1))

cm_plotlabels = ['defective','ok']
plot_cm(cm9, classes = cm_plotlabels )

